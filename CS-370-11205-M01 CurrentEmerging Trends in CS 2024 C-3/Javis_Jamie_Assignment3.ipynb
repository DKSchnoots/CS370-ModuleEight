{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 2s 0us/step\n",
      "X_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 4,200,842\n",
      "Trainable params: 4,200,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 47s 1ms/step - loss: 1.7432 - accuracy: 0.3810 - val_loss: 1.4655 - val_accuracy: 0.4817\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 55s 1ms/step - loss: 1.3864 - accuracy: 0.5111 - val_loss: 1.2879 - val_accuracy: 0.5444\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 50s 1ms/step - loss: 1.2532 - accuracy: 0.5595 - val_loss: 1.1613 - val_accuracy: 0.5978\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 47s 1ms/step - loss: 1.1596 - accuracy: 0.5896 - val_loss: 1.1260 - val_accuracy: 0.6133\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 46s 1ms/step - loss: 1.0957 - accuracy: 0.6151 - val_loss: 1.1300 - val_accuracy: 0.6114\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 48s 1ms/step - loss: 1.0394 - accuracy: 0.6371 - val_loss: 1.0668 - val_accuracy: 0.6345\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 47s 1ms/step - loss: 0.9874 - accuracy: 0.6565 - val_loss: 1.1306 - val_accuracy: 0.6078\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 52s 1ms/step - loss: 0.9423 - accuracy: 0.6772 - val_loss: 1.0211 - val_accuracy: 0.6522\n",
      "Epoch 9/20\n",
      "40000/40000 [==============================] - 50s 1ms/step - loss: 0.9036 - accuracy: 0.6839 - val_loss: 1.0164 - val_accuracy: 0.6484\n",
      "Epoch 10/20\n",
      "40000/40000 [==============================] - 46s 1ms/step - loss: 0.8642 - accuracy: 0.6969 - val_loss: 1.0441 - val_accuracy: 0.6415\n",
      "Epoch 11/20\n",
      "40000/40000 [==============================] - 47s 1ms/step - loss: 0.8318 - accuracy: 0.7135 - val_loss: 1.1139 - val_accuracy: 0.6238\n",
      "Epoch 12/20\n",
      "40000/40000 [==============================] - 47s 1ms/step - loss: 0.7980 - accuracy: 0.7226 - val_loss: 1.1192 - val_accuracy: 0.6261\n",
      "Epoch 13/20\n",
      "40000/40000 [==============================] - 47s 1ms/step - loss: 0.7710 - accuracy: 0.7315 - val_loss: 1.0596 - val_accuracy: 0.6601\n",
      "Epoch 14/20\n",
      "40000/40000 [==============================] - 49s 1ms/step - loss: 0.7429 - accuracy: 0.7412 - val_loss: 1.0129 - val_accuracy: 0.6688\n",
      "Epoch 15/20\n",
      "40000/40000 [==============================] - 53s 1ms/step - loss: 0.7217 - accuracy: 0.7512 - val_loss: 1.0507 - val_accuracy: 0.6541\n",
      "Epoch 16/20\n",
      "40000/40000 [==============================] - 48s 1ms/step - loss: 0.6934 - accuracy: 0.7615 - val_loss: 1.0406 - val_accuracy: 0.6697\n",
      "Epoch 17/20\n",
      "40000/40000 [==============================] - 47s 1ms/step - loss: 0.6715 - accuracy: 0.7675 - val_loss: 1.0704 - val_accuracy: 0.6725\n",
      "Epoch 18/20\n",
      "40000/40000 [==============================] - 48s 1ms/step - loss: 0.6530 - accuracy: 0.7743 - val_loss: 1.0407 - val_accuracy: 0.6800\n",
      "Epoch 19/20\n",
      "40000/40000 [==============================] - 47s 1ms/step - loss: 0.6339 - accuracy: 0.7797 - val_loss: 1.0818 - val_accuracy: 0.6656\n",
      "Epoch 20/20\n",
      "40000/40000 [==============================] - 48s 1ms/step - loss: 0.6125 - accuracy: 0.7876 - val_loss: 1.0803 - val_accuracy: 0.6802\n",
      "10000/10000 [==============================] - 6s 561us/step\n",
      "Test score: 1.0789563734054566\n",
      "Test accuracy: 0.6722000241279602\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# CIFAR_10 is a set of 60K images 32x32 pixels on 3 channels\n",
    "IMG_CHANNELS = 3\n",
    "IMG_ROWS = 32\n",
    "IMG_COLS = 32\n",
    "\n",
    "#constant\n",
    "BATCH_SIZE = 128\n",
    "NB_EPOCH = 20\n",
    "NB_CLASSES = 10\n",
    "VERBOSE = 1\n",
    "VALIDATION_SPLIT = 0.2\n",
    "OPTIM = RMSprop()\n",
    "\n",
    "#load dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "#convert to catergorical\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES) \n",
    "\n",
    "#float and normalization\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "#network\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "  \n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "#train\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIM,\n",
    "metrics=['accuracy'])\n",
    " \n",
    "model.fit(X_train, Y_train, batch_size=BATCH_SIZE,\n",
    "epochs=NB_EPOCH, validation_split=VALIDATION_SPLIT, \n",
    "verbose=VERBOSE)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test,\n",
    "batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "#save model\n",
    "model_json = model.to_json()\n",
    "open('cifar10_architecture.json', 'w').write(model_json)\n",
    "model.save_weights('cifar10_weights.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This algorithm could result in ethical and privacy concerns if it were trained on \n",
    "different sets of images is potential privacy concerns and bias. If the data set were different,\n",
    "it could potentially leak personal data or pick out certain identifying features that could leak\n",
    "a persons identity. Aside from sensative data being leaked, bias could also be a problem. \n",
    "Depending on the data presented, the algorithm might pick up bias. Lack of diversity depending \n",
    "on the images, could lead to misrepresenation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
