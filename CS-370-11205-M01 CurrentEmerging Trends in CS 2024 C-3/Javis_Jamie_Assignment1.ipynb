{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 9s 182us/step - loss: 1.4829 - accuracy: 0.6231 - val_loss: 0.7584 - val_accuracy: 0.8286\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 7s 152us/step - loss: 0.6049 - accuracy: 0.8464 - val_loss: 0.4550 - val_accuracy: 0.8852\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 16s 332us/step - loss: 0.4398 - accuracy: 0.8801 - val_loss: 0.3710 - val_accuracy: 0.9019\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 6s 131us/step - loss: 0.3767 - accuracy: 0.8952 - val_loss: 0.3322 - val_accuracy: 0.9082\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 8s 159us/step - loss: 0.3415 - accuracy: 0.9025 - val_loss: 0.3055 - val_accuracy: 0.9147\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 5s 100us/step - loss: 0.3175 - accuracy: 0.9086 - val_loss: 0.2880 - val_accuracy: 0.9182\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 4s 83us/step - loss: 0.2989 - accuracy: 0.9137 - val_loss: 0.2727 - val_accuracy: 0.9224\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 4s 87us/step - loss: 0.2839 - accuracy: 0.9180 - val_loss: 0.2608 - val_accuracy: 0.9266\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 4s 79us/step - loss: 0.2714 - accuracy: 0.9217 - val_loss: 0.2505 - val_accuracy: 0.9298\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 4s 91us/step - loss: 0.2602 - accuracy: 0.9252 - val_loss: 0.2430 - val_accuracy: 0.9308\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 4s 78us/step - loss: 0.2501 - accuracy: 0.9285 - val_loss: 0.2341 - val_accuracy: 0.9335\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 4s 82us/step - loss: 0.2409 - accuracy: 0.9301 - val_loss: 0.2271 - val_accuracy: 0.9352\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 4s 76us/step - loss: 0.2325 - accuracy: 0.9334 - val_loss: 0.2227 - val_accuracy: 0.9367\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 0.2253 - accuracy: 0.9353 - val_loss: 0.2147 - val_accuracy: 0.9396\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 9s 185us/step - loss: 0.2181 - accuracy: 0.9375 - val_loss: 0.2082 - val_accuracy: 0.9411\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 6s 134us/step - loss: 0.2116 - accuracy: 0.9394 - val_loss: 0.2030 - val_accuracy: 0.9431\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 6s 134us/step - loss: 0.2055 - accuracy: 0.9414 - val_loss: 0.1981 - val_accuracy: 0.9445\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 6s 123us/step - loss: 0.1996 - accuracy: 0.9430 - val_loss: 0.1932 - val_accuracy: 0.9458\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 4s 85us/step - loss: 0.1941 - accuracy: 0.9432 - val_loss: 0.1894 - val_accuracy: 0.9467\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 4s 88us/step - loss: 0.1890 - accuracy: 0.9456 - val_loss: 0.1849 - val_accuracy: 0.9498\n",
      "10000/10000 [==============================] - 0s 47us/step\n",
      "Test score: 0.18599770209044217\n",
      "Test accuracy: 0.9463000297546387\n"
     ]
    }
   ],
   "source": [
    "#Test One\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "# network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "# M_HIDDEN hidden layers\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/5\n",
      "48000/48000 [==============================] - 9s 177us/step - loss: 1.5284 - accuracy: 0.6085 - val_loss: 0.7963 - val_accuracy: 0.8273\n",
      "Epoch 2/5\n",
      "48000/48000 [==============================] - 7s 142us/step - loss: 0.6185 - accuracy: 0.8482 - val_loss: 0.4710 - val_accuracy: 0.8793\n",
      "Epoch 3/5\n",
      "48000/48000 [==============================] - 6s 131us/step - loss: 0.4466 - accuracy: 0.8826 - val_loss: 0.3841 - val_accuracy: 0.8980\n",
      "Epoch 4/5\n",
      "48000/48000 [==============================] - 7s 136us/step - loss: 0.3823 - accuracy: 0.8958 - val_loss: 0.3427 - val_accuracy: 0.9046\n",
      "Epoch 5/5\n",
      "48000/48000 [==============================] - 7s 144us/step - loss: 0.3465 - accuracy: 0.9034 - val_loss: 0.3163 - val_accuracy: 0.9124\n",
      "10000/10000 [==============================] - 0s 45us/step\n",
      "Test score: 0.31681152710318566\n",
      "Test accuracy: 0.9124000072479248\n"
     ]
    }
   ],
   "source": [
    "#Test Two\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "# network and training\n",
    "NB_EPOCH = 5\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "# M_HIDDEN hidden layers\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/40\n",
      "48000/48000 [==============================] - 7s 147us/step - loss: 1.3976 - accuracy: 0.6579 - val_loss: 0.6982 - val_accuracy: 0.8401\n",
      "Epoch 2/40\n",
      "48000/48000 [==============================] - 6s 116us/step - loss: 0.5778 - accuracy: 0.8502 - val_loss: 0.4432 - val_accuracy: 0.8837\n",
      "Epoch 3/40\n",
      "48000/48000 [==============================] - 4s 74us/step - loss: 0.4335 - accuracy: 0.8814 - val_loss: 0.3695 - val_accuracy: 0.8991\n",
      "Epoch 4/40\n",
      "48000/48000 [==============================] - 4s 83us/step - loss: 0.3764 - accuracy: 0.8955 - val_loss: 0.3344 - val_accuracy: 0.9059\n",
      "Epoch 5/40\n",
      "48000/48000 [==============================] - 4s 79us/step - loss: 0.3439 - accuracy: 0.9033 - val_loss: 0.3105 - val_accuracy: 0.9117\n",
      "Epoch 6/40\n",
      "48000/48000 [==============================] - 4s 79us/step - loss: 0.3212 - accuracy: 0.9093 - val_loss: 0.2935 - val_accuracy: 0.9161\n",
      "Epoch 7/40\n",
      "48000/48000 [==============================] - 4s 74us/step - loss: 0.3031 - accuracy: 0.9137 - val_loss: 0.2791 - val_accuracy: 0.9207\n",
      "Epoch 8/40\n",
      "48000/48000 [==============================] - 3s 71us/step - loss: 0.2883 - accuracy: 0.9183 - val_loss: 0.2672 - val_accuracy: 0.9234\n",
      "Epoch 9/40\n",
      "48000/48000 [==============================] - 3s 72us/step - loss: 0.2759 - accuracy: 0.9213 - val_loss: 0.2569 - val_accuracy: 0.9268\n",
      "Epoch 10/40\n",
      "48000/48000 [==============================] - 4s 89us/step - loss: 0.2647 - accuracy: 0.9243 - val_loss: 0.2487 - val_accuracy: 0.9281\n",
      "Epoch 11/40\n",
      "48000/48000 [==============================] - 4s 82us/step - loss: 0.2546 - accuracy: 0.9276 - val_loss: 0.2397 - val_accuracy: 0.9306\n",
      "Epoch 12/40\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.2451 - accuracy: 0.9299 - val_loss: 0.2334 - val_accuracy: 0.9342\n",
      "Epoch 13/40\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.2367 - accuracy: 0.9327 - val_loss: 0.2276 - val_accuracy: 0.9352\n",
      "Epoch 14/40\n",
      "48000/48000 [==============================] - 4s 88us/step - loss: 0.2290 - accuracy: 0.9345 - val_loss: 0.2203 - val_accuracy: 0.9371\n",
      "Epoch 15/40\n",
      "48000/48000 [==============================] - 5s 94us/step - loss: 0.2216 - accuracy: 0.9370 - val_loss: 0.2132 - val_accuracy: 0.9398\n",
      "Epoch 16/40\n",
      "48000/48000 [==============================] - 4s 91us/step - loss: 0.2148 - accuracy: 0.9386 - val_loss: 0.2077 - val_accuracy: 0.9417\n",
      "Epoch 17/40\n",
      "48000/48000 [==============================] - 4s 94us/step - loss: 0.2084 - accuracy: 0.9403 - val_loss: 0.2025 - val_accuracy: 0.9434\n",
      "Epoch 18/40\n",
      "48000/48000 [==============================] - 5s 104us/step - loss: 0.2023 - accuracy: 0.9424 - val_loss: 0.1985 - val_accuracy: 0.9433\n",
      "Epoch 19/40\n",
      "48000/48000 [==============================] - 5s 102us/step - loss: 0.1966 - accuracy: 0.9435 - val_loss: 0.1945 - val_accuracy: 0.9458\n",
      "Epoch 20/40\n",
      "48000/48000 [==============================] - 6s 121us/step - loss: 0.1914 - accuracy: 0.9450 - val_loss: 0.1895 - val_accuracy: 0.9467\n",
      "Epoch 21/40\n",
      "48000/48000 [==============================] - 4s 89us/step - loss: 0.1861 - accuracy: 0.9463 - val_loss: 0.1855 - val_accuracy: 0.9487\n",
      "Epoch 22/40\n",
      "48000/48000 [==============================] - 4s 91us/step - loss: 0.1811 - accuracy: 0.9484 - val_loss: 0.1832 - val_accuracy: 0.9481\n",
      "Epoch 23/40\n",
      "48000/48000 [==============================] - 4s 78us/step - loss: 0.1767 - accuracy: 0.9492 - val_loss: 0.1792 - val_accuracy: 0.9492\n",
      "Epoch 24/40\n",
      "48000/48000 [==============================] - 4s 85us/step - loss: 0.1724 - accuracy: 0.9505 - val_loss: 0.1756 - val_accuracy: 0.9503\n",
      "Epoch 25/40\n",
      "48000/48000 [==============================] - 5s 100us/step - loss: 0.1681 - accuracy: 0.9515 - val_loss: 0.1729 - val_accuracy: 0.9513\n",
      "Epoch 26/40\n",
      "48000/48000 [==============================] - 4s 85us/step - loss: 0.1641 - accuracy: 0.9532 - val_loss: 0.1692 - val_accuracy: 0.9521\n",
      "Epoch 27/40\n",
      "48000/48000 [==============================] - 4s 85us/step - loss: 0.1603 - accuracy: 0.9538 - val_loss: 0.1667 - val_accuracy: 0.9528\n",
      "Epoch 28/40\n",
      "48000/48000 [==============================] - 4s 79us/step - loss: 0.1566 - accuracy: 0.9550 - val_loss: 0.1637 - val_accuracy: 0.9532\n",
      "Epoch 29/40\n",
      "48000/48000 [==============================] - 4s 77us/step - loss: 0.1530 - accuracy: 0.9562 - val_loss: 0.1608 - val_accuracy: 0.9547\n",
      "Epoch 30/40\n",
      "48000/48000 [==============================] - 4s 78us/step - loss: 0.1497 - accuracy: 0.9575 - val_loss: 0.1588 - val_accuracy: 0.9542\n",
      "Epoch 31/40\n",
      "48000/48000 [==============================] - 4s 83us/step - loss: 0.1464 - accuracy: 0.9581 - val_loss: 0.1558 - val_accuracy: 0.9552\n",
      "Epoch 32/40\n",
      "48000/48000 [==============================] - 4s 79us/step - loss: 0.1432 - accuracy: 0.9590 - val_loss: 0.1547 - val_accuracy: 0.9557\n",
      "Epoch 33/40\n",
      "48000/48000 [==============================] - 4s 91us/step - loss: 0.1403 - accuracy: 0.9604 - val_loss: 0.1518 - val_accuracy: 0.9570\n",
      "Epoch 34/40\n",
      "48000/48000 [==============================] - 5s 112us/step - loss: 0.1373 - accuracy: 0.9607 - val_loss: 0.1510 - val_accuracy: 0.9578\n",
      "Epoch 35/40\n",
      "48000/48000 [==============================] - 5s 110us/step - loss: 0.1346 - accuracy: 0.9613 - val_loss: 0.1475 - val_accuracy: 0.9582\n",
      "Epoch 36/40\n",
      "48000/48000 [==============================] - 7s 139us/step - loss: 0.1318 - accuracy: 0.9622 - val_loss: 0.1460 - val_accuracy: 0.9587\n",
      "Epoch 37/40\n",
      "48000/48000 [==============================] - 6s 117us/step - loss: 0.1293 - accuracy: 0.9630 - val_loss: 0.1441 - val_accuracy: 0.9582\n",
      "Epoch 38/40\n",
      "48000/48000 [==============================] - 7s 146us/step - loss: 0.1266 - accuracy: 0.9638 - val_loss: 0.1431 - val_accuracy: 0.9591\n",
      "Epoch 39/40\n",
      "48000/48000 [==============================] - 5s 98us/step - loss: 0.1243 - accuracy: 0.9644 - val_loss: 0.1401 - val_accuracy: 0.9595\n",
      "Epoch 40/40\n",
      "48000/48000 [==============================] - 4s 84us/step - loss: 0.1218 - accuracy: 0.9649 - val_loss: 0.1387 - val_accuracy: 0.9599\n",
      "10000/10000 [==============================] - 0s 47us/step\n",
      "Test score: 0.1332351804148406\n",
      "Test accuracy: 0.9607999920845032\n"
     ]
    }
   ],
   "source": [
    "#Test Three\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "# network and training\n",
    "NB_EPOCH = 40\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "# M_HIDDEN hidden layers\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "For this assignment, I choose to modify the NB_EPOCH parameter values. \n",
    "Test One is the baseline from the book.\n",
    "Test Two is the test where I lower the value of NB_EPOCH to 5.\n",
    "Test Three is the test where I raise the value of NB_EPOCH to 40.\n",
    "\n",
    "In Test Two, lowering the NB_EPOCH, the model has less time to learn from data, \n",
    "we can see this based on the decrease in the final accuracy score.\n",
    "Lower accuracy implies that the model is negatively affected by the shortened period\n",
    "\n",
    "In Test Three, raising the NB_EPOCH, the model has more time to learn from data, \n",
    "we can see this based on the increase in the final accuracy score.\n",
    "Higher accuracy implies that the model is positively affected by the extended period"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
